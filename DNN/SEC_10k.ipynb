{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e778b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sec_edgar_downloader\n",
      "  Downloading sec_edgar_downloader-5.0.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from sec_edgar_downloader) (2.32.3)\n",
      "Collecting pyrate-limiter>=3.6.0 (from sec_edgar_downloader)\n",
      "  Downloading pyrate_limiter-3.9.0-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->sec_edgar_downloader) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->sec_edgar_downloader) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->sec_edgar_downloader) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->sec_edgar_downloader) (2024.12.14)\n",
      "Downloading sec_edgar_downloader-5.0.3-py3-none-any.whl (14 kB)\n",
      "Downloading pyrate_limiter-3.9.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: pyrate-limiter, sec_edgar_downloader\n",
      "Successfully installed pyrate-limiter-3.9.0 sec_edgar_downloader-5.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sec_edgar_downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a096c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found latest filing: sec-edgar-filings/NVDA/10-Q/0001012870-99-001954/full-submission.txt\n",
      "Item 1A section not found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sec_edgar_downloader import Downloader\n",
    "\n",
    "# ✅ MAIN FOLDER STRUCTURE CONFIG\n",
    "DATA_DIR = \"sec-edgar-filings\"\n",
    "\n",
    "# ✅ 1. Download latest filings into your target folder\n",
    "def download_latest_filing(ticker: str = \"NVDA\", form_type: str = \"10-Q\") -> str:\n",
    "    # Ensure the base directory exists\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    dl = Downloader(\"dvschenone@ucsd.edu\", DATA_DIR)\n",
    "    dl.get(form_type, ticker)\n",
    "\n",
    "    return get_latest_filing_file(ticker, form_type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ✅ 2. Traverse folders to find latest primary-document\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_latest_filing_file(ticker: str, form_type: str, year_threshold: int = 2020) -> str:\n",
    "    filings_path = os.path.join(DATA_DIR, ticker, form_type)\n",
    "    latest_file = None\n",
    "    latest_mtime = 0\n",
    "\n",
    "    if not os.path.exists(filings_path):\n",
    "        print(f\"❌ Path does not exist: {filings_path}\")\n",
    "        return None\n",
    "\n",
    "    for root, _, files in os.walk(filings_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".html\") or file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                mtime = os.path.getmtime(file_path)\n",
    "                file_year = datetime.fromtimestamp(mtime).year\n",
    "\n",
    "                if file_year < year_threshold:\n",
    "                    continue  # skip files before your cutoff\n",
    "\n",
    "                if mtime > latest_mtime:\n",
    "                    latest_file = file_path\n",
    "                    latest_mtime = mtime\n",
    "\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ✅ 3. Extract \"Item 1A. Risk Factors\" from .html or .txt\n",
    "def extract_item_1a(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # If it's HTML, parse and extract just the text\n",
    "    if \"<html\" in content.lower():\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "    else:\n",
    "        text = content\n",
    "\n",
    "    # Normalize and extract Item 1A section\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    pattern = r'(?i)(item\\s+1A\\.?\\s*[-–:]?\\s*Risk\\s+Factors)(.*?)(item\\s+1B\\.?|item\\s+2\\.)'\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        return match.group(2).strip()\n",
    "    else:\n",
    "        return \"Item 1A section not found.\"\n",
    "\n",
    "# ✅ 4. Example run\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = get_latest_filing_file(\"NVDA\", \"10-Q\", year_threshold=2020)\n",
    "\n",
    "    if file_path:\n",
    "        print(f\"✅ Found latest filing: {file_path}\")\n",
    "        print(extract_item_1a(file_path)[:1000])\n",
    "    else:\n",
    "        print(\"❌ Still no filing found.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a7d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
